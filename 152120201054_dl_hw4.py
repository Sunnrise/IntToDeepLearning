# -*- coding: utf-8 -*-
"""152120201054_DL_Hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wi3ogrbxJaPl4nmOGcz_kSsnQnExOttV
"""

import zipfile

# FÄ±rsly I added a zipfile from computer.
zip_path = '/content/HW4-CNN.zip'

# Specify the extraction directory
extraction_path = '/content/HW4-CNN/'

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extraction_path)
    # /content/extracted_folder/CaltechTinySplit then I carry files from here to main folder list (test and train)

pip install torch torchvision scikit-learn netron

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from sklearn.metrics import confusion_matrix, f1_score
import numpy as np

# Define the CNN model with Swish activation
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.swish = nn.SiLU()
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(64 * 16 * 16, 9)
    def forward(self, x):
        x = self.swish(self.conv1(x))
        x = self.pool(x)
        x = torch.flatten(x, 1)  # Flatten
        x = self.fc(x)
        return x

# Load and transform the dataset
transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])


train_set = datasets.ImageFolder(root='/content/HW4-CNN/CaltechTinySplit/train', transform=transform)
test_set = datasets.ImageFolder(root='/content/HW4-CNN/CaltechTinySplit/test', transform=transform)
val_set = datasets.ImageFolder(root='/content/HW4-CNN/CaltechTinySplit/val', transform=transform)

train_loader = DataLoader(train_set, batch_size=16, shuffle=True)
val_loader = DataLoader(val_set, batch_size=16, shuffle=False)
test_loader = DataLoader(test_set, batch_size=16, shuffle=False)

# Initialize the model, loss function, and optimizer
model = CNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Train the model
num_epochs = 60  # Number of epochs
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Print statistics every 10 batches
        running_loss += loss.item()
        if i % 10 == 9:  # Print every 10 batches
            print(f"[Epoch {epoch + 1}, Batch {i + 1}/{len(train_loader)}] Loss: {running_loss / 10:.3f}")
            running_loss = 0.0

# Save the model
torch.save(model.state_dict(), 'model.pth')

# Load and evaluate the model
model.load_state_dict(torch.load('model.pth'))
model.eval()

all_labels = []
all_preds = []
with torch.no_grad():
    for data in test_loader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        all_preds.extend(predicted.numpy())
        all_labels.extend(labels.numpy())

# Calculate confusion matrix and F1 score
conf_matrix = confusion_matrix(all_labels, all_preds)
f1 = f1_score(all_labels, all_preds, average='weighted')
print('Confusion Matrix:\n', conf_matrix)
print('F1 Score:', f1)